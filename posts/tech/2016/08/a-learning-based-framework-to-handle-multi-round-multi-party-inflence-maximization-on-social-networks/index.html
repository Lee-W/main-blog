
<!DOCTYPE html>

<html lang="zh-tw">
<head>
<meta charset="utf-8"/>
<meta charset="utf-8" content="text/html" http-equiv="Content-Type">
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
<title>[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks </title>
<meta content="True" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="origin" name="referrer"/>
<meta content="Pelican" name="generator"/>
<link href="https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" rel="canonical"/>
<!-- Feed -->
<link href="https://blog.wei-lee.me/feeds/all.atom.xml" rel="alternate" title="Those aren't written down are meant to be forgotten Full Atom Feed" type="application/atom+xml"/>
<!-- TODO: combine -->
<link href="https://blog.wei-lee.me/theme/css/style.css" rel="stylesheet" type="text/css"/>
<link href="https://blog.wei-lee.me/theme/css/pygments.css" rel="stylesheet" type="text/css"/>
<link href="https://blog.wei-lee.me/theme/css/pagefind-ui-custom.css" rel="stylesheet" type="text/css"/>
<link href="https://blog.wei-lee.me/theme/css/code.css" rel="stylesheet" type="text/css"/>
<link href="https://blog.wei-lee.me/theme/css/code-copy.css" rel="stylesheet" type="text/css"/>
<link href="https://blog.wei-lee.me/theme/css/custom.css" rel="stylesheet" type="text/css"/>
<link href="/pagefind/pagefind-ui.css" rel="stylesheet"/>
<!-- Custom fonts -->
<link href="https://fonts.googleapis.com/css?family=Montserrat:400,300" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css"/>
<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->
<script>
    var siteUrl = 'https://blog.wei-lee.me';
  </script>
<script>
    var localTheme = localStorage.getItem('attila_theme');
    switch (localTheme) {
      case 'dark':
        document.documentElement.classList.add('theme-dark');
        break;
      case 'light':
        document.documentElement.classList.add('theme-light');
        break;
      default:
        break;
    }
  </script>
<meta content="Paper" name="description"/>
<meta content="Wei Lee" name="author"/>
<meta content="Paper" name="tags"/>
<meta content="Social Network" name="tags"/>
<meta content="Machine Learning" name="tags"/>
<meta content="Game Theory" name="tags"/>
<!-- Open Graph -->
<meta content="Those aren't written down are meant to be forgotten" prefix="og: http://ogp.me/ns#" property="og:site_name">
<meta content="[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks " prefix="og: http://ogp.me/ns#" property="og:title">
<meta content="Paper" prefix="og: http://ogp.me/ns#" property="og:description">
<meta content="zh-tw" prefix="og: http://ogp.me/ns#" property="og:locale">
<meta content="https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" prefix="og: http://ogp.me/ns#" property="og:url">
<meta content="article" prefix="og: http://ogp.me/ns#" property="og:type">
<meta content="2016-08-22 16:53:00+08:00" prefix="og: http://ogp.me/ns#" property="article:published_time"/>
<meta content="" prefix="og: http://ogp.me/ns#" property="article:modified_time"/>
<meta content="https://blog.wei-lee.me/author/wei-lee.html" prefix="og: http://ogp.me/ns#" property="article:author"/>
<meta content="Tech" prefix="og: http://ogp.me/ns#" property="article:section">
<meta content="Paper" prefix="og: http://ogp.me/ns#" property="article:tag"/>
<meta content="Social Network" prefix="og: http://ogp.me/ns#" property="article:tag"/>
<meta content="Machine Learning" prefix="og: http://ogp.me/ns#" property="article:tag"/>
<meta content="Game Theory" prefix="og: http://ogp.me/ns#" property="article:tag"/>
<meta content="https://blog.wei-lee.me//images/cover.jpeg" prefix="og: http://ogp.me/ns#" property="og:image"/>
<!-- Twitter Card -->
<meta content="summary_large_image" name="twitter:card"/>
<meta content="@clleew" name="twitter:site"/>
<meta content="[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks " name="twitter:title"/>
<meta content="https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" name="twitter:url"/>
<meta content="https://blog.wei-lee.me//images/cover.jpeg" name="twitter:image:src"/>
<meta content="Paper" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks",
  "headline": "[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks",
  "datePublished": "2016-08-22 16:53:00+08:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Wei Lee",
    "url": "https://blog.wei-lee.me/author/wei-lee.html"
  },
  "image": "https://blog.wei-lee.me//images/cover.jpeg",
  "url": "https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks",
  "description": "Paper"
}
</script>
</meta></meta></meta></meta></meta></meta></meta></link></meta><link href="https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "Those aren't written down are meant to be forgotten", "item": "https://blog.wei-lee.me"}, {"@type": "ListItem", "position": 2, "name": "Posts", "item": "https://blog.wei-lee.me/posts"}, {"@type": "ListItem", "position": 3, "name": "Tech", "item": "https://blog.wei-lee.me/posts/tech"}, {"@type": "ListItem", "position": 4, "name": "2016", "item": "https://blog.wei-lee.me/posts/tech/2016"}, {"@type": "ListItem", "position": 5, "name": "08", "item": "https://blog.wei-lee.me/posts/tech/2016/08"}, {"@type": "ListItem", "position": 6, "name": "A learning based framework to handle multi round multi party inflence maximization on social networks", "item": "https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks"}, {"@type": "ListItem", "position": 7, "name": "Index", "item": "https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks/index.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "Wei Lee"}, "publisher": {"@type": "Organization", "name": "Those aren't written down are meant to be forgotten"}, "headline": "[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks", "about": "Tech", "datePublished": "2016-08-22 16:53"}</script></head>
<body class="category-template">
<div class="nav-header">
<nav aria-label="Main" class="nav-wrapper">
<ul>
<li class="nav-Home" role="presentation"><a href="/"><span>Home</span></a></li>
<li class="nav-About Me" role="presentation"><a href="/pages/about-me.html"><span>About Me</span></a></li>
<li class="nav-üóÑÔ∏è Archives" role="presentation"><a href="/archives.html"><span>üóÑÔ∏è Archives</span></a></li>
<li class="nav-üîç Search" role="presentation"><a href="/pages/search.html"><span>üîç Search</span></a></li>
<li role="presentation"><a href="https://blog.wei-lee.me/category/book.html"><span>Book</span></a></li>
<li role="presentation"><a href="https://blog.wei-lee.me/category/gossiping.html"><span>Gossiping</span></a></li>
<li class="nav-tech active" role="presentation"><a href="https://blog.wei-lee.me/category/tech.html"><span>Tech</span></a></li>
</ul>
<ul class="nav-meta">
<li class="nav-linkedin"><a aria-label="Linkedin" href="https://tw.linkedin.com/in/clleew" target="_blank"><i aria-hidden="true" class="icon icon-linkedin"></i>
<span>Linkedin</span>
</a></li>
<li class="nav-github"><a aria-label="Github" href="https://github.com/Lee-W" target="_blank"><i aria-hidden="true" class="icon icon-github"></i>
<span>GitHub</span>
</a></li>
<li class="nav-twitter"><a aria-label="Twitter" href="https://twitter.com/clleew" target="_blank"><i aria-hidden="true" class="icon icon-twitter"></i>
<span>Twitter</span>
</a></li>
<li class="nav-search" style="display: none;">
<a title="Search">
<i aria-hidden="true" class="icon icon-search"></i>
<span>Search</span>
</a>
</li>
</ul> </nav>
<div class="nav-wrapper-control">
<div class="inner">
<a class="nav-menu" role="button"><i aria-hidden="true" class="icon icon-menu"></i>Menu</a>
<a class="nav-search" role="button" style="display: none;" title="Search"><i aria-hidden="true" class="icon icon-search"></i></a>
</div>
</div>
</div>
<div aria-label="Close" class="nav-close" role="button"></div>
<section class="page-wrapper" id="wrapper">
<!-- Progressbar -->
<div class="progress-container">
<span class="progress-bar"></span>
</div>
<!-- Page Header -->
<!-- Set your background image for this header on the line below. -->
<header class="post-header has-cover">
<div class="inner">
<span class="post-info">
<span class="post-type">Article</span>
<span class="post-count">Tech</span>
</span>
<h1 class="post-title">
            [Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks
        </h1>
<div class="post-meta">
<div class="post-meta-avatars">
<figure class="post-meta-avatar avatar">
<a class="author-avatar" href="https://blog.wei-lee.me/author/wei-lee.html">
<img alt="Wei Lee" class="author-profile-image" src="https://blog.wei-lee.me//images/avatar.jpeg"/>
</a>
</figure>
</div>
<h4 class="post-meta-author">
            Wei Lee
          </h4>
<time datetime="2016/08/22 - Mon">2016/08/22 - Mon</time>
          ‚Ä¢ 4 min read
        </div>
<div class="post-cover cover">
<img alt="Category Tech" src="https://blog.wei-lee.me//images/cover.jpeg"/>
</div>
</div>
</header>
<!-- Post content -->
<main class="content" data-pagefind-body="" role="main">
<article class="post">
<div class="inner">
<section class="post-content">
<p><a href="http://dl.acm.org/citation.cfm?id=2783392">Paper</a></p>
<!--more-->
<div class="toc">
<ul>
<li><a href="#1-introduction">1. Introduction</a><ul>
<li><a href="#difference-with-others">Difference with Others</a></li>
</ul>
</li>
<li><a href="#2-problem-statement">2. Problem Statement</a><ul>
<li><a href="#def-1-competitive-linear-threshold-clt">Def 1: Competitive Linear Threshold (CLT)</a></li>
<li><a href="#def-2-multi-round-competitive-influence-maximization-mrcim">Def 2: Multi-Round Competitive Influence Maximization (MRCIM)</a></li>
</ul>
</li>
<li><a href="#4-methodology">4. Methodology</a><ul>
<li><a href="#41-preliminary-reinforcement-learning">4.1 Preliminary: Reinforcement Learning</a></li>
<li><a href="#42-strategy-oriented-reinforcement-learning">4.2 Strategy-Oriented Reinforcement-Learning</a></li>
</ul>
</li>
<li><a href="#43-storm-with-strategy-known">4.3 STORM with Strategy Known</a></li>
<li><a href="#44-storm-with-strategy-unknown">4.4 STORM with Strategy Unknown</a><ul>
<li><a href="#unknown-but-available-to-train">Unknown but available to train</a></li>
<li><a href="#unknown">Unknown</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="1-introduction">1. Introduction</h2>
<ul>
<li>
<p>Problem Description</p>
<ul>
<li>A company intends to select a small set of customers to distribute praises of their trial products to a larger group</li>
</ul>
</li>
<li>
<p>Influence maximization</p>
<ul>
<li>Goal: Identify a small subset of seed nodes that have the best chance to influence the most number of nodes</li>
<li>Competitive Influence Maximization (CIM)</li>
</ul>
</li>
<li>
<p>Assumption</p>
<ul>
<li>Influence is exclusive (Once a node is influenced by one party, it will not be influenced again)</li>
<li>Each round all parties choose one node and then the influence propagates before the next round starts</li>
</ul>
</li>
<li>
<p>STORM (STrategy-Oriented Reinforcement-Learning based influence Maximization) performs</p>
<ul>
<li>Data Generation<ul>
<li>the data, which is the experience generated through simulation by applying the current model, will become the feedbacks to refine the model for better performance</li>
</ul>
</li>
<li>Model Learning</li>
</ul>
</li>
</ul>
<h3 id="difference-with-others">Difference with Others</h3>
<ol>
<li>Known strategy ‚Üí Both know and unknown<ul>
<li>Known or Unknown but available to compete ‚Üí Train a model to learn strategy</li>
<li>Unknown ‚Üí Game-theoretical solution to seek the Nash equilibrium</li>
</ul>
</li>
<li>Single-roung ‚Üí Multi-round</li>
<li>Model driven ‚Üí learning-based, data-drivern</li>
<li>Not considering different network topology ‚Üí General to adapt both opponent's strategy and environment setting (e.g. underlying network topology)</li>
</ol>
<h2 id="2-problem-statement">2. Problem Statement</h2>
<h3 id="def-1-competitive-linear-threshold-clt">Def 1: Competitive Linear Threshold (CLT)</h3>
<ul>
<li>CLT model is a multi-party diffusion model</li>
<li>The party who has the highest influence occupied the node</li>
</ul>
<h3 id="def-2-multi-round-competitive-influence-maximization-mrcim">Def 2: Multi-Round Competitive Influence Maximization (MRCIM)</h3>
<ul>
<li>Max its overall relative influence</li>
</ul>
<h2 id="4-methodology">4. Methodology</h2>
<ul>
<li>NP-hardness of MRCIM ‚Üí looks for approxmiate solution</li>
<li>Max the inflence for each round does not guarantee overall max<ul>
<li>Due to the fact that each round are not independent</li>
</ul>
</li>
</ul>
<h3 id="41-preliminary-reinforcement-learning">4.1 Preliminary: Reinforcement Learning</h3>
<ul>
<li>Learn a policy <span class="math">\(\pi(s)\)</span> to determine which action to take state s (environment)</li>
<li>How to estimated <span class="math">\(\pi\)</span>?<ul>
<li>Expected Accmulated Reward of a state (V function)<ul>
<li><span class="math">\( V^\pi(s) = E_\pi\{R_t|S_t=s\}=...\)</span></li>
</ul>
</li>
<li>Expected Accmulated Reward of a state-action pair (Q function)<ul>
<li><span class="math">\( Q^\pi(s, a) = E_\pi\{R_t|S_t=s, a_t=a\}=...\)</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The optimal <span class="math">\(\pi\)</span> can be obtained through Q functinon</p>
<p><span class="math">\( \pi = \arg \min_{a\in A}Q(s,a)\)</span></p>
<p>(i.e. For all "a" in A, find the "a" such that min Q(s, a))</p>
<h3 id="42-strategy-oriented-reinforcement-learning">4.2 Strategy-Oriented Reinforcement-Learning</h3>
<h4 id="setup">Setup</h4>
<ul>
<li>Env<ul>
<li>Influence propagation process</li>
</ul>
</li>
<li>Reward<ul>
<li>Delay Reward: The difference of activated nodes between parties at the last round<ul>
<li>After the last round, rewards are propagated to the previous states through Q-function updating</li>
<li>Slow but more accurate</li>
</ul>
</li>
</ul>
</li>
<li>Action<ul>
<li><del>Choosing certain node to activate</del><ul>
<li>too many</li>
<li>overfit</li>
</ul>
</li>
<li>Single Party IM strategies<ul>
<li>Namely, which strategy to choose given the current state</li>
<li>The size can be reduced to strategies chosen</li>
<li>Chosen Strategies<ul>
<li>sub-greedy</li>
<li>degree-first</li>
<li>block</li>
<li>max-weight</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>State<ul>
<li>Represents<ul>
<li>network</li>
<li>environment status</li>
</ul>
</li>
<li><del>record the occupation status of all nodes</del><ul>
<li><span class="math">\(3^{|V|}\)</span>, too many</li>
<li>overfit</li>
</ul>
</li>
<li>Features Designed<ul>
<li>Number of free nodes</li>
<li>Sum of degrees of all nodes</li>
<li>Sum of weight of the edges for which bot h vertices are free</li>
<li>Max degree among all free nodes</li>
<li>Max sum of free out-edge weight of a node among nodes which are the first player's neighbors</li>
<li>Second player's</li>
<li>Max activated nodes of a node for the first player alter two rounds of influence propagation</li>
<li>Second player's</li>
</ul>
</li>
<li>The feautres are quantize into<ul>
<li>low</li>
<li>medium</li>
<li>high</li>
</ul>
</li>
<li>Totally, <span class="math">\(3^9\)</span> states</li>
</ul>
</li>
</ul>
<h4 id="data-for-training">Data For Training</h4>
<ul>
<li>Propagation model is known (e.g. LT in the experiments)</li>
<li>Strategies served as actions are predefined</li>
</ul>
<p>In training phase, train the agent against a certain strategy and see how it performs on the given network<br/>
These data can be used to learn the value functions</p>
<h4 id="training-against-opponents">Training Against Opponents</h4>
<ul>
<li>Opponent Strategy<ul>
<li>Known: Simulate the strategy during training</li>
<li>Unknown but available during training: Same as above</li>
<li>Unknown: More General Model in 4.4</li>
</ul>
</li>
</ul>
<h4 id="phase">Phase</h4>
<ul>
<li>Phase 1: Training<ul>
<li>The agent update its Q function from the simulation experiences throughout the training rounds</li>
<li>Update <span class="math">\(\pi\)</span> in the meantime</li>
</ul>
</li>
<li>Phase 2: Competition<ul>
<li>The agent would not update Q-table</li>
<li>Generates <span class="math">\(\pi\)</span> according to Q-table</li>
</ul>
</li>
</ul>
<h2 id="43-storm-with-strategy-known">4.3 STORM with Strategy Known</h2>
<ul>
<li>Training the model compete against the strategy to learn <span class="math">\(\pi\)</span></li>
<li>STORM-Q<ul>
<li>Update Q-function following the concept of Q-learning<ul>
<li>Q-Learning: <span class="math">\(Q(S_t, a_t) = Q(S_t, a_t) + \alpha * (r_{t+1} + \gamma * max_{a}Q(S_{t+1}, a) -Q(S_t, a_t))\)</span></li>
</ul>
</li>
<li><span class="math">\(\epsilon\)</span>-greedy<ul>
<li>Determine strategies on the current policy derived from Q-table.</li>
<li>Explore the new directions to avoid local optimum</li>
</ul>
</li>
<li>Pure Strategy<ul>
<li>The most likely strategy is chosen</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>$ Algorithm $</p>
<h2 id="44-storm-with-strategy-unknown">4.4 STORM with Strategy Unknown</h2>
<h3 id="unknown-but-available-to-train">Unknown but available to train</h3>
<ul>
<li>The differece between the known case is that experience cannot be obtained through simulation</li>
<li>Train against unknown opponent's strategy during competition<ul>
<li>It's feasible because STORM-Q only needs to know the seed-selection outcoms of the opponent to update the Q-table, not exact strategy it takes</li>
</ul>
</li>
</ul>
<h3 id="unknown">Unknown</h3>
<ul>
<li>Goal: Create a general model to compete a variety of rational strategies</li>
<li>Assumption: The oppoent is rational (Wants to max influence and knows its oppoent wants so)</li>
<li>STORM-QQ<ul>
<li>Two STROM-Q compete and update Q-tabale at the same time</li>
<li>Using current Q-table during training phase</li>
<li>Pure Strategy<ul>
<li>Does Not guarantee that equilibrium exists in MRCIM</li>
</ul>
</li>
</ul>
</li>
<li>
<p>STORM-MM</p>
<ul>
<li>Mix Strategy (Samples an action from the distribution of actions in each state)</li>
<li>In two-player zero-sum game<ul>
<li>Nash equilibrium is graranteed to exist with miexed strategies</li>
<li>Use MINMAX theorem to find the equilibrium</li>
</ul>
</li>
<li><span class="math">\(Q(s, a, o)\)</span>: The reward of first party when using strategy <span class="math">\(a\)</span> against oppoent's strategy <span class="math">\(o\)</span> in state <span class="math">\(s\)</span></li>
<li><span class="math">\(Q_{t+1}(s_t, a_t, o_t) = (1-\alpha)Q_t(s_t, a_t, o_t)+\alpha[r_{t+1}+\gamma V(s_{t+1})]\)</span></li>
<li>Operations  Research</li>
</ul>
</li>
<li>
<p>The differece between STROM-QQ and STORM-MM</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>STROM-QQ</th>
<th>STROM-MM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Max the reward in their own Q-table</td>
<td>Finds equilibrium with one Q-table and determines both side's <span class="math">\(a\)</span> at the same time</td>
</tr>
<tr>
<td>Pure Strategies</td>
<td>Mixed Strategies</td>
</tr>
<tr>
<td>Choose strategy by greedy</td>
<td>Samples from the mixed strategy <span class="math">\(\pi_a\)</span> or <span class="math">\(\pi_o\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Ideally, they should have similar result in two-party MRCIM. In practice, the result might not due to<ul>
<li>STORM-QQ does not guarantee equilibrium</li>
<li>Although equilibrium exists in STORM-MM. It does not guarantee to be found due to lack of training data or bad init or such problems.</li>
</ul>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</section>
<section class="post-footer">
<div class="post-share">
<span class="post-info-label">Share</span>
<a aria-label="Twitter" class="twitter" href="https://twitter.com/share?text=[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks &amp;url=https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Twitter">
<i aria-hidden="true" class="icon icon-twitter"></i><span class="hidden">Twitter</span>
</a>
<a aria-label="Facebook" class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Facebook">
<i aria-hidden="true" class="icon icon-facebook"></i><span class="hidden">Facebook</span>
</a>
<a aria-label="LinkedIn" class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks&amp;title=[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks" onclick="window.open(this.href, 'linkedin-share', 'width=930,height=720');return false;" title="LinkedIn">
<i aria-hidden="true" class="icon icon-linkedin"></i><span class="hidden">LinkedIn</span>
</a>
<a aria-label="Email" class="email" href="mailto:?subject=[Paper] A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks&amp;body=https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" title="Email">
<i aria-hidden="true" class="icon icon-mail"></i><span class="hidden">Email</span>
</a>
<div class="clear"></div>
</div>
<aside class="post-tags">
<a href="https://blog.wei-lee.me/tag/paper.html">Paper</a><a href="https://blog.wei-lee.me/tag/social-network.html">Social Network</a><a href="https://blog.wei-lee.me/tag/machine-learning.html">Machine Learning</a><a href="https://blog.wei-lee.me/tag/game-theory.html">Game Theory</a> </aside>
<div class="clear"></div>
</section>
<script type="text/javascript">var use_utterance = true;</script>
<section class="post-comments">
<script async="" crossorigin="anonymous" data-issue-term="https://blog.wei-lee.me/posts/tech/2016/08/a-learning-based-framework-to-handle-multi-round-multi-party-inflence-maximization-on-social-networks" data-label="blog-comment" data-repo="Lee-W/main-blog" data-theme="github-light" src="https://utteranc.es/client.js">
</script>
</section>
<aside class="post-nav">
<div class="clear"></div>
</aside>
</div>
</article>
</main>
<div class="nav-footer">
<nav aria-label="Footer" class="nav-wrapper">
<span class="nav-copy">Those aren't written down are meant to be forgotten ¬© 2023
          <a class="nav-rss" href="https://blog.wei-lee.me/feeds/all.atom.xml" target="_blank" title="RSS"><i class="icon icon-rss"></i></a>
</span>
<span class="nav-credits">



          Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a> ‚Ä¢ Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a> ‚Ä¢
          <a class="menu-item js-theme" data-dark="Dark theme" data-light="Light theme" data-system="System theme" href="#">
<span class="theme-icon"></span><span class="theme-text">System theme</span>
</a>
</span>
</nav>
</div>
</section>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
<!-- TODO: minify -->
<script src="https://blog.wei-lee.me/theme/js/jquery.fitvids.js" type="text/javascript"></script>
<script src="https://blog.wei-lee.me/theme/js/script.js" type="text/javascript"></script>
<script src="https://blog.wei-lee.me/theme/js/clipboard.min.js" type="text/javascript"></script>
<script src="https://blog.wei-lee.me/theme/js/copy-to-clipboard.js" type="text/javascript"></script>
<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-3J7XQ46QH5"></script>
<script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-3J7XQ46QH5', { 'anonymize_ip': true });
    </script>
<script src="/pagefind/pagefind-ui.js"></script>
<script>
      window.addEventListener('DOMContentLoaded', (event) => {
          new PagefindUI({
              element: "#search",
              showImages: false,
          });
      });
  </script>
<!-- 	The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in article.html, but it needs to be included down here, after jQuery has already loaded. -->
<script>
  $(document).ready(function () {
    var viewport = $(window);
    var post = $('.post-content');
    // Responsive videos with fitVids
    post.fitVids();

    // Reading progress bar on window top
    function readingProgress() {
      var postBottom = post.offset().top + post.height();
      var viewportHeight = viewport.height();
      var progress = 100 - (((postBottom - (viewport.scrollTop() + viewportHeight) + viewportHeight / 3) / (postBottom - viewportHeight + viewportHeight / 3)) * 100);
      $('.progress-bar').css('width', progress + '%');
      (progress > 100) ? $('.progress-container').addClass('complete'): $('.progress-container').removeClass('complete');
    }
    readingProgress();
    // Trigger reading progress
    viewport.on({
      'scroll': function() {
        readingProgress();
      },
      'resize': function() {
        readingProgress();
      },
      'orientationchange': function() {
        readingProgress();
      }
    });

  });
</script>
</body>
</html>