Title: AIP-72 - Task Execution Interface aka Task SDK
Subtitle: Airflow çš„ task è¦æ”¯æ´ Python ä»¥å¤–çš„èªè¨€äº†ï¼
Date: 2024-11-06 19:15
Category: Tech
Tags: Python, Airflow, Airflow 3.0, Airflow Improvement Proposal, Digest
Slug: aip-72
Authors: Wei Lee
Series: å¸¶æˆ‘è®€ AIP

~~ä»¥å¾Œ Flyte ä¸èƒ½å†ä»¥é€™é»å˜´ Airflow äº† ğŸ˜†~~

<!--more-->

[TOC]

## å‰è¨€
AIP çš„å…¨åæ˜¯ [Airflow Improvement Proposals][AIP]
é¡§åæ€ç¾©ï¼Œè·Ÿ Python Enhancement Proposal (PEP) å·®ä¸å¤š
å°±æ˜¯è®“ Airflow è®Šå¾—æ›´å¥½çš„ææ¡ˆ

æœ€è¿‘é–‹ç™¼ [AIP-75] çš„åŠŸèƒ½ç™¼ç¾ï¼Œæœ‰äº› AIP çš„å…§å®¹æˆ‘çœ‹éåˆå¿˜äº†
ä¸å°å¿ƒåˆå•äº†è ¢å•é¡Œ
æ­£å¦‚åŒæˆ‘éƒ¨è½å€‹ç¾åœ¨çš„æ¨™é¡Œ "Those aren't written down are meant to be forgotten"
æ²’å¯«ä¸‹ä¾†çš„æ±è¥¿ï¼Œæœ€çµ‚éƒ½æœƒå¿˜è¨˜çš„
é‚£å°±ä¾†åšå€‹ç­†è¨˜å§

ç³»åˆ—åã€Œå¸¶æˆ‘è®€ AIPã€ç•¶ç„¶æ˜¯æä»– COSCUP çš„ã€Œå¸¶æ‚¨è®€æºç¢¼ã€
ä½ ä¹Ÿè¨±æœƒå•ï¼Œé‚£ä¸æ˜¯æ‡‰è©²è¦æ˜¯ã€Œå¸¶æ‚¨è®€ AIPã€å—ï¼Ÿ
é€™å•é¡Œå•å¾—å¥½
å› ç‚ºé€™æ˜¯å¯«çµ¦æˆ‘è‡ªå·±çš„ç­†è¨˜
ä¸éå¦‚æœå‰›å¥½å°ä½ æœ‰å¹«åŠ©çš„è©±å°±å¤ªå¥½äº† ğŸ˜‡
è®€èµ·ä¾†æ‡‰è©²å°±è·Ÿæˆ‘çš„æ›¸æ‘˜ç³»åˆ—å·®ä¸å¤š

æœ€è¿‘è¦é–‹å§‹åš [Move Asset user-facing components to task_sdk #43619][airflow-issue-43619]
æŠŠ [AIP-74] æ‰€æ”¹åçš„ Asset çš„ç›¸é—œæ±è¥¿ç§»å‹•åˆ° [AIP-72] æ‰€æå‡ºçš„ TaskSDK
æ‰€ä»¥é€™ç³»åˆ—çš„æ–‡ç« å°±å¾ [AIP-72] é–‹å§‹äº†

## Note
* [Link to AIP-72][AIP-72]

### Definitions
* Airflow Core: the scheduler and API servers
* User code: DAG files, but not necessarily plugins

### Motivation
* Reduces the interaction between the Task (user code) and Airflow Core â†’ **Airflow Core and Task can be upgraded separately**
* Avoid workers/tasks to Airflow DB access while still being able to get the information required â†’ task executions on local, trusted networks
* Interface for remote workers to connect to Airflow Core service for information needed for task execution â†’ better remote execution
* Language agnostic interface â†’ **allowing Tasks in other languages**
    * **DAGs will still be Python-based**

### Changes to make
* No extra components outside of Airflow and its metadata DB should be needed to run Airflow locally

#### Disable Direct DB access
* Remove direct DB access from
    * user code
    * workers
    * triggerers
    * triggers
* **All DB traffic goes through an API server**
    * reduce the number of open connections to DB

#### Build a new API for task communication from the ground up
* Endpoints for all items in the "Airflow Task SDK"
* Strongly versioned using **CalVer**
* Supports WebSockets or other "instant" push for low-latency
    * in practice, Async web framework (e.g., FastAPI)
* Transport encoding: TBD

#### API security and Strong per-task-try identity
* Strong identity for each **task try** â† authorization
    * current choice: JWT token signed by the API server

#### Define an "Airflow Task SDK"
* Interface for
    * Version introspection
    * Airflow Connections
    * Variables
    * Read and write
        * XCom
        * Triggers
    * heartbeat
    * logs
    * metrics
    * OpenLineage events
    * data about task expansion for downstream tasks
        * values
        * lengths
        * names
        * etc.
    * etc.
* Access to the Connections and Variables (detail TBD)
* Read/Write task log or XCom
    * consideration point
        * avoid requiring every log line for every task execution to go over the API server due to performance
        * solution TBD
* Out of scope
    * handling custom callbacks

#### Extend Executor interface
* Replace ~~`queue_task_instance()`, `queue_command()`, `send_callback()`~~ with `queue_activity()`

```python
def queue_activity(
    self,
    *,
    # For now just "ExecuteTask", "RunCallback", but allows for "ParseDAGFile" etc. in future
    kind: ActivityType,
    # Contains TI for "ExecuteTask", so executor can get ti.queue out of this.
    context: dict[str, Any],
): ...
```

* Key difference: what's passed between the scheduler and worker
    * from ~~commands to run~~ to *better typed parameters*

```json
{
  "kind": "ExecuteTask",
  "token": "eyJhb....",
  "id": [ "dag_id", "task_id", "run_id", "attempt", "map_index"], # Or better, a single id, but we want these details to be passed along regardless
  "context": {  
        "connections": ["list", "of", "pre-injected", "connections"],
        "...",
    }
}
```

#### Dag Parsing Changes
* Strong identity for the source file being parsed
* The same Task SDK during
    * dag parsing
    * task execution
* Callback details TBD
    * run in the parsing process and will need to be "passed down" to it

#### Connection/Variable security models
* a.k.a. push/inject secrets into tasks or allow tasks to request secrets.
* Details TBD

### Which users are affected by the change?
* Operators that needed direct DB access need re-writing

## Distro proposal based on the Task Execution Interface
é€™æ˜¯åœ¨ [Airflow dev mailing list](https://lists.apache.org/thread/gm2rj7s4vsnyt95bxbh1pjkht49zvb3t) è·Ÿ [AIP-72] é«˜åº¦ç›¸é—œçš„ææ¡ˆ
æ‰€ä»¥æˆ‘å°±ä¸€èµ·è®€äº†

* [link to the google doc](https://docs.google.com/document/d/1isjpMKYdgdYpEGQDtN2hKrJ-oq8qFWU9kqootRUHiX8/edit)

### Task Context: Incoming to Tasks
* Information obtained from the API server for task execution
    * Connections
    * Secrets
    * XCom
    * Variables
    * Log output location reference
    * Composite TaskRunID

### Task Status and Output: Outgoing from Tasks
* information sent to the API server
    * Tasks Completion status
    * XCom *(optional)*
    * Variables (write) *(optional)*
    * Log information *(optional)*
    * Heartbeat
        * The scheduler knows that this task is still running

### User scenarios
> Worth reading, but I'm going to skip it in my digest

### Distro proposal
1. Server Distribution (based on Python)
    * MetaDB handling
    * Scheduler / Executor
    * API server
    * Web server
    * DAG File processor
    * Executor providers (Celery / k8s / Edge)
    * Logging providers (*maybeâ“*)
    * Server providers (Secrets backend, AuthMgr backend, etc.) (*as needed*)
2. Python Task SDK (python distribution)
    * TEI serialization / deserialization
    * HTTP / REST client and token authentication capabilities
    * Default operators
    * Logging write provider (*maybeâ“*)
    * Providers (hooks / operators / sensors) (*as needed*)
3. Consolidated Distribution (1 + 2)
4. Golang Task SDK (Go distribution)
    * similar to 2 but in Go

[AIP]: https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals
[AIP-72]: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK
[AIP-74]: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-74+Introducing+Data+Assets
[AIP-75]: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-75+New+Asset-Centric+Syntax
[airflow-issue-43619]: https://github.com/apache/airflow/issues/43619
